# Principal component analysis {.section}

<!-- Allow color highlighting in mathjax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

```{r, echo=FALSE}
# Clear memory
rm(list=ls())

# simulate data
set.seed(1976)

if(!"DT" %in% installed.packages()) { install.packages("DT") }
library("DT")
```

## Principal component analysis {.smaller}

<img src="https://upload.wikimedia.org/wikipedia/commons/1/18/Karl_Pearson%2C_1912.jpg" style="height:200px; margin:0 20px 20px 0; float: left;">

<img src="https://upload.wikimedia.org/wikipedia/en/4/49/Harold_Hotelling.jpg" style="height:200px; margin:0 20px 20px 0; float: left;">

Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.

PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s.

Source: [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)

## Data reduction method {.flexbox .vcenter}

<iframe width="560" height="315" style="width:560px; height:315px;" src="https://www.youtube.com/embed/fIbAs_VXcqc?rel=0" frameborder="2" gesture="media" allowfullscreen></iframe>

## Reflective vs Formative {.flexbox .vcenter}

![](../../../topics/pca_cb_alpha/Reflective_Formative.png)

## Usage

Principal component analysis can be used in multiple ways.

* To use factor scores
* To solve multicollinearity
* To test construct validity

## Interpretation

* Eigenvalues
    * Criterion: Eigenvalue $\geq$ 1
* Cumulative explained variance
* Factor loadings

## Assumptions

* Good collinearity (correlation between items)
    * Not to high (singular)
    * Not to low
* Bartlett's test (correlation should not only be with itself)
    * $H_0$: Indentity matrix (>.05 not good)
    * $H_A$: Non IM (<.05 is good)
* Normality of variables
* Adequacy of sample size
    * KMO < .5 not good
    * KMO of 1 is good
    * Both general and specific

## Varimax Rotation

<iframe width="560" height="315" style="width:560px; height:315px;" src="https://www.youtube.com/embed/njgD1AO0Z6k" frameborder="2" allowfullscreen></iframe>

[Interactive rotations](http://demonstrations.wolfram.com/PairwiseAxesRotationsInFactorAnalysis/) by Steve Hunka.

## Simulate latent traits {.subsection}

* Conscientiousness
* Extraversion
* Neuroticism

```{r}
n     = 100
mu    = 0 
sigma = 2

# Create latent traits
trait.con <- rnorm(n,mu,sigma)
trait.ext <- rnorm(n,mu,sigma)
trait.neu <- rnorm(n,mu,sigma) 
```

---------

![Local independence](../../../topics/pca_cb_alpha/conditional_indep.png)


## Latent traits are uncorrelated {.subsection}

```{r}
cor(cbind(trait.con,
          trait.ext,
          trait.neu))
```

## Set number of items per trait

```{r}
n.trait.con = 6
n.trait.ext = 5
n.trait.neu = 7

data <- data.frame(id = 1:n)
```

## Items for traits 1

```{r}
sigma.internal.inconsistency = 6
data$item.1 <- trait.con + rnorm(n,10,sigma.internal.inconsistency)
data$item.2 <- trait.con + rnorm(n,10,sigma.internal.inconsistency)
data$item.3 <- trait.con + rnorm(n,10,sigma.internal.inconsistency)
data$item.4 <- trait.con + rnorm(n,10,sigma.internal.inconsistency)
data$item.5 <- trait.con + rnorm(n,10,sigma.internal.inconsistency)
data$item.6 <- trait.con + rnorm(n,10,sigma.internal.inconsistency)
```

## Correlations trait 1

```{r}
plot(cbind(trait.con,data[,2:7]))
```

## Items for traits 2

```{r}
sigma.internal.inconsistency = 1
data$item.7  <- trait.ext + rnorm(n,10,sigma.internal.inconsistency)
data$item.8  <- trait.ext + rnorm(n,10,sigma.internal.inconsistency)
data$item.9  <- trait.ext + rnorm(n,10,sigma.internal.inconsistency)
data$item.10 <- trait.ext + rnorm(n,10,sigma.internal.inconsistency)
data$item.11 <- trait.ext + rnorm(n,10,sigma.internal.inconsistency)
```

## Correlations trait 2

```{r}
plot(cbind(trait.ext,data[,8:12]))
```

## Items for traits 3

```{r}
sigma.internal.inconsistency = 1
data$item.12 <- trait.neu + rnorm(n,10,sigma.internal.inconsistency)
data$item.13 <- trait.neu + rnorm(n,10,sigma.internal.inconsistency)
data$item.14 <- trait.neu + rnorm(n,10,sigma.internal.inconsistency)
data$item.15 <- trait.neu + rnorm(n,10,sigma.internal.inconsistency)
data$item.16 <- trait.neu + rnorm(n,10,sigma.internal.inconsistency)
data$item.17 <- trait.neu + rnorm(n,10,sigma.internal.inconsistency)
data$item.18 <- trait.neu + rnorm(n,10,sigma.internal.inconsistency)
```

## Correlations trait 3

```{r}
plot(cbind(trait.neu,data[,13:19]))
```

## Have a look at the data {.smaller}

```{r, echo=FALSE}
data <- round(abs(data), 2)

datatable(data, rownames = FALSE, options = list(searching = FALSE, scrollY = 415, paging = F, info = F))
```

```{r, echo=FALSE}
# Write data for use in SPSS
write.csv(data, "factors.csv", row.names=FALSE)
```

## Correlations trait 1

```{r, echo=FALSE, eval=FALSE}
plot(cbind(trait.con,data[ , 2:7 ],
           trait.ext,data[ , 8:12],
           trait.neu,data[ ,13:19]))
```

```{r}
cor(cbind(trait.con,
          trait.ext,
          trait.neu,
          data[ ,2:7]))[-c(1:3),1:3]
```

## Correlations trait 2

```{r}
cor(cbind(trait.con,
          trait.ext,
          trait.neu,
          data[ ,8:12]))[-c(1:3),1:3]
```

## Correlations trait 3

```{r}
cor(cbind(trait.con,
          trait.ext,
          trait.neu,
          data[ ,13:19]))[-c(1:3),1:3]
```

## Show local independence {.subsection}

Item 7 and 8 are positively correlated.

```{r}
cor(data$item.7, data$item.8)
```

![Local independence](../../../topics/pca_cb_alpha/conditional_indep_small.png)

--------

But this correlation dissapears if we control for the latent variable.

```{r}
r.xy = cor(data$item.7, data$item.8)
r.xz = cor(data$item.7, trait.ext)
r.yz = cor(data$item.8, trait.ext)
pcor = ( r.xy - (r.xz * r.yz) ) / ( sqrt(1-r.xz^2) * sqrt(1-r.yz^2) )
pcor
```

# Classical Test Theory  

## CTT {.section}

In Dutch: "Klassieke Test Theorie (KTT)"

Classical test theory (CTT) is a body of related psychometric theory that predicts outcomes of psychological testing such as the difficulty of items or the ability of test-takers. It is a theory of testing based on the idea that a personâ€™s observed or obtained score on a test is the sum of a true score (error-free score) and an error score. Generally speaking, the aim of classical test theory is to understand and improve the reliability of psychological tests.

Classical test theory as we know it today was codified by Novick (1966) and described in classic texts such as Lord & Novick (1968) and Allen & Yen (1979/2002).

Source: [Wikipedia](https://en.wikipedia.org/wiki/Classical_test_theory)

## T = X + E

True score = Observed score + Error

$$T = X + E$$

```{r}
X = apply(data[,2:7], 1, sum)
T = trait.con
```

## Plot X and T

```{r, echo=FALSE}
plot(X,T)
```

## Explained variance {.subsection}

Indication of accuracy of estimations

<!-- $\rho_{XT}$ &rarr; $r_{XT}$ -->

```{r}
cor(X,T)^2
```

## Error

```{r}
E = T - X
summary(scale(E))
```

## Cronbach's alpha {.subsection}

$$\alpha = \frac{N}{N-1} \left(1-\frac{\sum_{i=1}^N \sigma_{Y_i}^2}{\sigma_X^2}\right)$$

- $N$ = number of items. $K$ is also used some times
- $\sigma_{Y_i}^2$ = variances of individual items
- $\sigma_X^2$ = variance of observed sum score

This formula deviates from Field for it's a clearer notation and corresponds to use in earlier courses.

## CB $\alpha$ calculations {.subsection}

$$\definecolor{red}{RGB}{255,0,0}
\alpha = \frac{N}{N-1} \left(1-\frac{\color{red}\sum_{i=1}^N \sigma_{Y_i}^2}{\sigma_X^2}\right)$$

```{r}
# Calculate variance per column
variance.per.column <- apply(data[,2:7], 2, var)

# Sum of variance per column
sum.var <- sum(variance.per.column)

sum.var
```

## CB $\alpha$ calculations

$$\alpha = \frac{N}{N-1} \left(1-\frac{\sum_{i=1}^N \sigma_{Y_i}^2}{\color{red} \sigma_X^2}\right)$$

```{r}
# Calculate sum score per row
sum.score <- apply(data[,2:7], 1, sum)

# Calculate variance of sum score
var.sum.score <- var(sum.score)

var.sum.score
```

## Calculate variance component {.subsection}

This represents the proportion of item variance to total variance.

$$\alpha = \frac{N}{N-1} \left(1-\color{red}\frac{\sum_{i=1}^N \sigma_{Y_i}^2}{\sigma_X^2}\right)$$

```{r}
prop.item.total.var <- sum.var / var.sum.score

prop.item.total.var
```

## Calculate proportion covariance {.subsection}

$$\alpha = \frac{N}{N-1} \color{red}\left(1-\frac{\sum_{i=1}^N \sigma_{Y_i}^2}{\sigma_X^2}\right)$$

```{r}
prop.covar <- 1 - prop.item.total.var
```

## Show relation sumscore variance {.smaller .subsection}

Here we show the relation of the variance in the sumscore to the variance and covariance of the individual items.

$$
\Sigma
=
\begin{bmatrix}
    \sigma^2_{11} & \sigma^2_{12} & \dots  & \sigma^2_{1n} \\
    \sigma^2_{21} & \sigma^2_{22} & \dots  & \sigma^2_{2n} \\
    \vdots        & \vdots        & \ddots & \vdots \\
    \sigma^2_{d1} & \sigma^2_{d2} & \dots  & \sigma^2_{dn}
\end{bmatrix} = \sigma_{sumscore}^2
$$

```{r}
cov(data[,2:7])
```

----------

```{r}
sum(cov(data[,2:7])); var(sum.score)
sum(cov(data[,2:7])) == var(sum.score)
```

## Covariance {.smaller}

```{r}
cov(data[,2:7])
variance.per.column
```

## Covariance {.smaller}

So the covariance can be found off diagonal.

```{r}
cov(data[,2:7])*(1-diag(6))
```

---------

Where the proportion of covariance to the total variance is:

```{r}
sum(cov(data[,2:7])*(1-diag(6))) / sum(cov(data[,2:7]))
```

Wich is equal to:

```{r}
prop.covar
```


$$\alpha = \frac{N}{N-1} \color{red}\left(1-\frac{\sum_{i=1}^N \sigma_{Y_i}^2}{\sigma_X^2}\right)$$

## Calculate CB $\alpha$

$$\alpha = \frac{N}{N-1} \left(1-\frac{\sum_{i=1}^N \sigma_{Y_i}^2}{\sigma_X^2}\right)$$

```{r}
# Set number of items
N = 6

# Calculate Cronbach's Alpha
cb.alpha = (N/(N-1)) * prop.covar

cb.alpha
```