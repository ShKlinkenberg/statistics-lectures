# Multiple regression {.section}

## Multiple regression {.smaller}

<small>

$$\LARGE{\text{outcome} = \text{model} + \text{error}}$$

In statistics, linear regression is a linear approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables denoted X.

$$\LARGE{Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dotso + \beta_n X_{ni} + \epsilon_i}$$

In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters $\beta$'s are estimated from the data.

Source: [wikipedia](https://en.wikipedia.org/wiki/Linear_regression)

</small>

## Outcome vs Model

```{r, fig.align='center', fig.asp=.5}
error = c(2, 1, .5, .1)
n = 100

layout(matrix(1:4,1,4))
for(e in error) {
  
  x = rnorm(n)
  y = x + rnorm(n, 0 , e)
  
  r   = round(cor(x,y), 2)
  r.2 = round(r^2, 2)
  
  plot(x,y, las = 1, ylab = "outcome", xlab = "model", main = paste("r =", r," r2 = ", r.2), ylim=c(-2,2), xlim=c(-2,2))
  fit <- lm(y ~ x)
  abline(fit, col = "red")
  
}

```

## Assumptions {.subsection}

A selection from Field (8.3.2.1. Assumptions of the linear model):

For simple regression

* Sensitivity
* Homoscedasticity

For multiple regressin

* Multicollinearity
* Linearity

## Sensitivity

Outliers

* Extreme residuals
    * Cook's distance (< 1)
    * Mahalonobis (< 11 at N = 30)
    * Laverage (The average leverage value is defined as (k + 1)/n)
    
```{r, fig.align='center', fig.width=5, fig.height=5, echo=FALSE}
x[1] =  2
y[1] = -2

plot(x,y, 
     las  = 1, 
     ylab = "outcome", 
     xlab = "model", 
     main = paste("r =", r," r2 = ", r.2), 
     ylim = c(-2,2), 
     xlim = c(-2,2))
  
abline(fit, col = "red")

fit2 <- lm(y ~ x)
abline(fit2, col = "blue")
  
r   = round(cor(x,y), 2)
r.2 = round(r^2, 2)

text(x[1], y[1], paste("r =", r," r2 = ", r.2), pos = 2, col = "blue")
```

## Homoscedasticity

* Variance of residual should be equal across all expected values
     * Look at scatterplot of standardized: expected values $\times$ residuals. Roughly round shape is needed.

```{r, fig.align='center', fig.width=5, fig.height=5, echo=TRUE}
fit <- lm(x[2:n] ~ y[2:n])

ZPRED  = scale(fit$fitted.values)
ZREDID = scale(fit$residuals)

plot(ZPRED, ZREDID)
abline(h = 0, v = 0, lwd=2)

#install.packages("plotrix")
if(!"plotrix" %in% installed.packages()) { install.packages("plotrix") };
library("plotrix")
draw.circle(0,0,1.7,col=rgb(1,0,0,.5))
```


## Multicollinearity

<img class="shadow" style="float:right; width:250px;" src="../../../../topics/regression_multiple_predictors/pinguins.jpg">

To adhere to the multicollinearity assumptien, there must not be a too high linear relation between the predictor variables.

This can be assessed through:

* Correlations
* Matrix scatterplot
<!-- * VIF: max < 10, mean < 1 -->
* Tolerance > 0.2

## Linearity

For the linearity assumption to hold, the predictors must have a linear relation to the outcome variable.

This can be checked through:

* Correlations
* Matrix scatterplot with predictors and outcome variable

## Example

Perdict study outcome based on IQ and motivation.

## Read data {.smaller}

```{r, echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
set.seed(273645)
n = 89

latent.variable = rnorm(n, 0, 1) # Brain speed?

IQ        = 120 + ( latent.variable * 15 ) + rnorm(n)
motivation = 15 + ( latent.variable * 2  ) + rnorm(n)

b_0 = -100
b_1 = 1 # Beta for IQ̧
b_2 = 1 # Beta for Motivation

error = rnorm(n, 0, 5)

study.outcome = b_0 + b_1 * IQ + b_2 * motivation + error

data = data.frame(study.outcome, motivation, IQ)

fit = lm(study.outcome ~ motivation + IQ, data)
summary(fit)

head(data, 10)
```


```{r, warning=FALSE, message=FALSE}
data <- read.csv('IQ.csv', header=T)

head(data)

IQ            = data$IQ
study.outcome = data$Studieprestatie
motivation    = data$Motivatie
```

## Regression model in R 

Perdict study outcome based on IQ and motivation.

```{r}
fit <- lm(study.outcome ~ IQ + motivation)
```

## What is the model {.subsection .smaller}

```{r}
fit$coefficients

b.0 = round(fit$coefficients[1], 2) ## Intercept
b.1 = round(fit$coefficients[2], 2) ## Beta coefficient for IQ
b.2 = round(fit$coefficients[3], 2) ## Beta coefficient for motivation
```

De beta coëfficients are: 

* $b_0$ (intercept) = `r b.0`
* $b_1$ = `r b.1`
* $b_2$ = `r b.2`.

## Model summaries

```{r}
summary(fit)
```


## Visual

```{r, echo=FALSE, warning=FALSE}
## 3d plot package rgl
## install.packages('rgl')
if(!"rgl" %in% installed.packages()) { install.packages("rgl") };
library("rgl")

library(knitr)
knit_hooks$set(webgl = hook_webgl)



#1 #D scatter 

plot3d(IQ, motivation, study.outcome,
       col  = "red",
       size = 8)

#2 Add IQ value planes 

quantiles <- as.vector(quantile(IQ,seq(.1,.9,.1)))
sds       <- c(mean(motivation)+(sd(motivation)*c(-1,0,1)))  
sds       <- 1:4

planes3d(a = 0,
         b = 1,
         c = 0,
         #d = c(1,2,3,4),
         d = -sds,
         alpha=0.7,
         color = c("blue"))

#3 Add regression model surface

## Fit model
fit <- lm(study.outcome ~ IQ + motivation)

## Create xyz coordinates
regeq <- function(IQ, motivation) { 
    fit$coefficients[1] + 
    fit$coefficients[2]*IQ + 
    fit$coefficients[3]*motivation
}


x.pre <- seq(120, 135,length.out=30)
y.mod <- seq( 0,   5 ,length.out=30)

z.pre <- outer(x.pre, y.mod, FUN='regeq')

# z.pre[z.pre > 4] = 4
# z.pre[z.pre < 1] = 1


## Add 3D regression plane to scatter plot 
surface3d(x.pre, y.mod, z.pre, color = c("green"))

aspect3d(1,1,1)
# view3d(theta = 5)
# view3d(theta = -10, phi = -90)
# play3d(spin3d(axis = c(0, 0, 1), rpm = 30), duration = 5)
rglwidget()
```

## What are the expected values based on this model

$$\widehat{\text{studie prestatie}} = b_0 + b_1 \text{IQ} + b_2 \text{motivation}$$

```{r}
exp.stu.prest = b.0 + b.1 * IQ + b.2 * motivation

model = exp.stu.prest
```

$$\text{model} = \widehat{\text{studie prestatie}}$$

## Apply regression model {.subsection}

$$\widehat{\text{studie prestatie}} = b_0 + b_1 \text{IQ} + b_2 \text{motivation}$$
$$\widehat{\text{model}} = b_0 + b_1 \text{IQ} + b_2 \text{motivation}$$

```{r}
cbind(model, b.0, b.1, IQ, b.2, motivation)[1:5,]
```

$$\widehat{\text{model}} = `r b.0` + `r b.1` \times \text{IQ} + `r b.2` \times \text{motivation}$$

## How far are we off?

```{r}
error = study.outcome - model

cbind(model, study.outcome, error)[1:5,]
```

## Outcome = Model + Error {.subsection}

Is that true?

```{r}
study.outcome == model + error
```

> - Yes!

## Visual {.smaller .subsection}

```{r}
plot(study.outcome, xlab='personen', ylab='study.outcome')

n = length(study.outcome)
gemiddelde.study.outcome = mean(study.outcome)

## Voeg het gemiddelde toe
lines(c(1, n), rep(gemiddelde.study.outcome, 2))

## Wat is de totale variantie?
segments(1:n, study.outcome, 1:n, gemiddelde.study.outcome, col='blue')

## Wat zijn onze verwachte scores op basis van dit regressie model?
points(model, col='orange')

## Hoever zitten we ernaast, wat is de error?
segments(1:n, study.outcome, 1:n, model, col='purple', lwd=3)
```

## Explained variance {.subsection .smaller}

<small>

The explained variance is the deviation of the estimated model outcome compared to the total mean.

To get a percentage of explained variance, it must be compared to the total variance. In terms of squares:

$$\frac{{SS}_{model}}{{SS}_{total}}$$

We also call this: $r^2$ of $R^2$.

```{r}
r = cor(study.outcome, model)
r^2
```

</small>

## Compare models

```{r}
fit1 <- lm(study.outcome ~ motivation, data)
fit2 <- lm(study.outcome ~ motivation + IQ, data)

# summary(fit1)
# summary(fit2)

anova(fit1, fit2)
```

